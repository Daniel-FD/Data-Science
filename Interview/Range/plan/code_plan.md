Introduction

Blockchain transaction analysis involves examining cryptocurrency transfers to discover patterns, unusual activities, and relationships between participants. In this project, we analyze a dataset of Solana blockchain payments to cluster addresses by behavior, detect anomalies, and study the transaction network. This can reveal hidden insights because all blockchain transactions are public, allowing data scientists to spot patterns and potential fraud that would otherwise remain unseen ￼ ￼. In simple terms, we want to understand who is transacting with whom, identify groups of addresses with similar habits, catch any strange or out-of-the-ordinary transactions, and visualize the web of transactions as a network.

The dataset contains individual token transfers with the following fields (columns) for each transaction record:
	•	sender – the blockchain address that sends the tokens (like the payer).
	•	recipient – the address that receives the tokens (like the payee).
	•	amount – the quantity of cryptocurrency tokens transferred (often in the smallest unit of the token).
	•	token – the type of cryptocurrency token being transferred (analogous to currency code, e.g. different tokens like USDC, SOL, etc., similar to how USD or EUR denote currencies ￼).
	•	height – the block height at which the transaction was recorded. This acts as a timestamp substitute (each new block has a sequential height and blocks are created ~every 400ms on Solana ￼, so a higher height means a later time).
	•	tx_hash – the transaction hash, a unique identifier for the transaction. Multiple transfers sharing the same tx_hash were part of one single transaction executed together ￼ (for example, one user sending funds to several people at once will appear as multiple rows with the same tx_hash).

A network graph of blockchain transactions, illustrating one address (center, in orange) sending cryptocurrency to many others (blue nodes). Each circle represents an address, and an arrow represents a transaction from sender to recipient. Such visualizations help identify “hubs” (highly connected addresses) and understand the overall network structure.

By analyzing this data step-by-step, we aim to identify clusters of addresses with similar behavior, flag any anomalous transactions (e.g. extremely large transfers or sudden bursts of activity), and build a transaction graph to find key players in the network. This plan will outline how to approach the coding for each of these tasks in a clear, beginner-friendly way.

Folder Structure and Filenames

Organizing the project into a clear folder structure makes the analysis easier to manage. Below is a breakdown of the recommended folders and Python script files for this project:
	•	data/ – Directory for the dataset and any other data files.
	•	transfers_sample.jsonl – The raw dataset file containing the transaction records (sender, recipient, amount, token, height, tx_hash).
	•	scripts/ – Directory for all Python scripts that perform the analysis steps. Each script focuses on a specific part of the workflow (described in detail later):
	•	load_data.py – Script to load the raw data from the JSONL file.
	•	preprocess_data.py – Script to clean and preprocess the data (prepare it for analysis).
	•	feature_engineering.py – Script to create new features (columns/metrics) from the raw transaction data for further analysis.
	•	clustering_analysis.py – Script to perform clustering on addresses and profile each cluster’s characteristics.
	•	anomaly_detection.py – Script to detect anomalous transactions or address behaviors.
	•	network_analysis.py – Script to build the transaction graph and perform network analysis.
	•	outputs/ – (Optional) Folder to store results produced by the scripts, such as files with cluster labels, lists of detected anomalies, or network metrics.
	•	notebooks/ – (Optional) If using Jupyter notebooks for exploration or visualization, they can be stored here (e.g., an interactive notebook combining all steps for reporting).
	•	README.md – A markdown file explaining the project and how to run the code (helpful for collaborators or future reference).

This structure separates each major task into its own script, making the code easier to understand and maintain. Next, we’ll go through the implementation plan step by step, following this structure.

Step-by-Step Implementation Plan

Now we break down the project into a sequence of steps. Each step corresponds to a part of the analysis and will be handled by one or more of the scripts mentioned above. We’ll explain what each step entails and how to implement it in simple terms:
	1.	Load and Preprocess the Data – The first step is to read in the transaction data and get it ready for analysis. Using a Python script (e.g. load_data.py), we will load the JSON Lines file (transfers_sample.jsonl) into a suitable data structure (like a pandas DataFrame). Then, we’ll clean and preprocess the data (preprocess_data.py). Preprocessing includes converting data types (e.g. amount and height from strings to numeric types), handling missing or malformed entries (if any), and sorting transactions by block height (so they are in chronological order). We might also derive a time indicator from the block height (knowing that blocks are 400ms apart ￼, we can roughly convert height differences into time intervals). The result of this step is a clean dataset in memory (or saved as a new file) that we can use for all subsequent analyses.
	2.	Engineer Useful Features from Transactions – Next, we create new features that summarize each address’s activity, a process called feature engineering. The raw dataset has detailed transaction records; from these, we will calculate higher-level metrics that help describe behavior. For example, for each unique address (particularly each sender or recipient), we can compute: the number of transactions they participated in (count of times seen as sender or recipient), the total value they sent and received, the average transaction amount, and the number of unique counterparties (how many different addresses they interacted with). We might also create features like max transaction amount (the largest single transfer an address made) or the frequency of transactions (e.g., transactions per day, if we convert block heights to an approximate timeline). If multiple token types exist in the data, we could include features per token (for instance, how much of each token an address sent). These features turn the data into a format where each address is described by a vector of numbers capturing its behavior. This will be the input for clustering and anomaly detection. (This step will be implemented in the feature_engineering.py script.)
	3.	Perform Clustering & Address Profiling – With the engineered features, we will group similar addresses together using a clustering algorithm. Clustering means automatically grouping data points that have similar characteristics ￼. In our case, the “data points” are addresses described by the features from the previous step. Addresses that behave similarly – for example, those that make transactions very frequently, or those that only occasionally move large amounts – should end up in the same group. We might use an algorithm like K-means clustering (which will partition addresses into K clusters based on feature similarity) or another clustering method suitable for our data. The goal is to discover natural groupings such as “frequent transactors,” “large-value movers,” or “mostly inactive” addresses ￼. After running the clustering in the clustering_analysis.py script, we will do address profiling: for each cluster, we examine its typical feature values to interpret what that cluster represents. For instance, we might find one cluster where addresses have a very high number of transactions but low average amount (perhaps “retail users”), versus another cluster with few transactions but very large amounts (“whale” accounts). We will print out or report these findings, describing each cluster in simple terms (e.g. Cluster 1: mostly low-activity accounts; Cluster 2: high-activity, small transactions; Cluster 3: high-value transfers, etc.). This helps turn the raw cluster results into meaningful insights about different user behaviors on the blockchain.
	4.	Detect Anomalies in Transactions – After clustering the typical behaviors, we focus on finding anomalies, i.e. transactions or addresses that deviate from the norm. Anomaly detection is the process of identifying rare or unusual data points that don’t fit the expected patterns ￼. In the context of blockchain transactions, anomalies could be outlier transactions (for example, an abnormally large transfer amount that is far higher than others ￼) or suspicious activity spikes (for example, an address that suddenly makes a huge number of transactions in a short period). Using the anomaly_detection.py script, we will implement checks for such outliers. One simple approach is to use statistical thresholds: for instance, we can compute the distribution of transaction amounts and flag any transaction beyond the 99th percentile (extremely high value) as a potential anomaly. Similarly, we can look at the features per address (from step 2) and identify addresses with unusually high values (e.g., an address with an extremely large total volume sent, or one that has an unusually large number of recipients compared to everyone else). We might use methods like the Z-score (how many standard deviations away from the mean a value is) or the Interquartile Range (IQR) method to detect outliers. The script will output a list of flagged anomalies – for example, it might print transactions with tx_hash and amount that are outliers, or addresses that stand out in terms of activity. Each detected anomaly will be reviewed and explained (e.g., “Address X appears to be an outlier because it sent 1000 SOL in one transaction, which is significantly higher than typical transactions in the dataset”). This step helps us pinpoint any irregular behavior that might indicate errors, special cases, or fraud.
	5.	Build a Transaction Graph & Network Analysis – The final analytical step is to construct a transaction graph from the data and analyze the network structure of addresses. In this graph, each node represents an address, and each edge (a link from one node to another) represents a transaction from the sender to the recipient ￼. We will use the network_analysis.py script to create this graph. First, we’ll iterate through all transactions in the data and add an edge for each transfer (for example, an edge from address A to address B if A sent funds to B). This will likely produce a directed graph (since transactions have a direction from sender to recipient) that may have thousands of nodes (addresses). Once the graph is built, we will compute basic network metrics to understand its properties. For instance, we’ll look at degree centrality – which addresses have the most connections? (In-degree could indicate an address that receives from many others, and out-degree an address that sends to many others.) Addresses with very high degree might be exchanges or popular services, acting as hubs in the network. We will also examine connected components – groups of addresses that are interconnected – to see if the transaction network is mostly one big cluster or if it breaks into smaller isolated sub-networks ￼. Another aspect is identifying influential addresses or “hubs”: for example, an address that appears in many transactions or connects to many different addresses could be considered influential ￼. We might calculate metrics like betweenness centrality or PageRank for a more advanced analysis of influence, but initially simply finding the top addresses by number of connections or total volume transacted can highlight the key players. After computing these metrics, the script will output results such as “Top 10 addresses by number of recipients they have sent to” or “Largest connected component contains X addresses.” We will interpret these findings to conclude which addresses are most central in the network and how the network is structured (e.g., does it have a hub-and-spoke structure, are there clear communities, etc.). This network analysis step provides a visual and structural perspective on the transaction data, complementing the statistical analyses from earlier steps.

Explanation of Each Script’s Function

Finally, let’s summarize what each Python script in the project does, to clarify the role of each piece of code in the overall analysis:
	•	load_data.py – Data Loading Script. This script’s job is to read the raw dataset file into a Python environment. It opens the transfers_sample.jsonl file (which contains one JSON-formatted transaction per line) and loads all the records into a pandas DataFrame or another convenient structure. The script will likely print out a summary of the data (such as the number of transactions loaded and the column names) to verify that the import was successful. In short, load_data.py gets the data ready for processing by other scripts.
	•	preprocess_data.py – Data Preprocessing Script. This script takes the raw DataFrame from the loading step and cleans it. It converts data types to the proper format (e.g., numerical types for amount and height), handles any missing or duplicate entries (for example, if there are any duplicated transactions or null values, it might drop or fix them), and sorts the data by height so that transactions are in chronological order. It might also create a new column for an approximate timestamp or time bucket if needed (using the block height information). The output of this script is a cleaned DataFrame (it could save this to a new file like cleaned_transactions.csv or simply pass it to the next step) with all transactions ready for analysis. Essentially, preprocess_data.py ensures the dataset is consistent and analysis-ready.
	•	feature_engineering.py – Feature Engineering Script. This script generates new features from the transaction data, focusing especially on aggregating information per address. It will likely iterate over all transactions and compile statistics for each unique address. For example, it can create a dictionary or DataFrame where each entry is an address and the columns are features like total_sent, total_received, count_sent, count_received, avg_tx_amount, unique_partners, etc., as discussed in the implementation plan. The script might use group-by operations in pandas to sum up amounts per address or count transactions. If multiple token types exist, it could also compute features per token or filter to a specific token depending on the analysis focus. The end result is a new dataset (perhaps saved as address_features.csv) where each row corresponds to an address and includes all the computed behavioral metrics for that address. These features will be used by subsequent scripts (clustering and anomaly detection). In summary, feature_engineering.py transforms raw transaction lists into meaningful features that describe each address’s activity.
	•	clustering_analysis.py – Clustering and Profiling Script. This script takes the address feature data (produced by the previous script) and applies a clustering algorithm to group addresses with similar features. Inside the script, we might use a library like scikit-learn to perform clustering (for example, using KMeans with a chosen number of clusters, or DBSCAN if we want the algorithm to find an appropriate number of clusters on its own). The script will standardize or normalize features if necessary (so that no single feature dominates due to scale differences) and then run the clustering algorithm. After clustering, the script will have a cluster label for each address. It will then perform address profiling: for each cluster, compute summary statistics of features (like the average and range of transactions counts, amounts, etc. within that cluster) and print out an interpretation. For instance, the script might output text such as “Cluster 0: 500 addresses, on average 5 transactions each, low volumes (avg 2 SOL) – likely low-activity users.” and “Cluster 1: 20 addresses, very high total sent (avg 100k SOL), high out-degree – likely exchanges or large traders.” The script may also save the cluster labels for each address to a file (e.g., cluster_labels.csv mapping address -> cluster) for reference. In essence, clustering_analysis.py finds groups of similar addresses and helps explain who those groups are.
	•	anomaly_detection.py – Anomaly Detection Script. This script scans through the transactions and the derived features to identify anything that stands out as unusual. It might operate in two modes: transaction-level anomalies and address-level anomalies. For transaction-level anomalies, the script will look at each transaction’s amount (and possibly the token type) to see if any amounts are extremely large or otherwise rare compared to the rest. It could sort transactions by amount and flag the top 0.1% as outliers, or use statistical techniques (like marking any transaction amount that is more than, say, 3 standard deviations above the mean as an anomaly). For address-level anomalies, the script will use the address features and find addresses with exceptional values (for example, an address that transacted with an unusually high number of other addresses, or an address that suddenly had a big spike in activity in a short span of block heights). The script will then output these findings, perhaps printing lists or saving them. For example, it might print: “Anomalously large transactions: tx_hash XYZ sent 5,000,000 tokens (most transactions are in the thousands range).” and “Anomalous address activity: Address ABC had 50 transactions in one block (spike in activity).” The goal of anomaly_detection.py is to automatically flag and report the outliers for further investigation, helping to ensure we don’t miss any irregular behavior in the data.
	•	network_analysis.py – Network Graph Analysis Script. This script constructs a graph of the transactions and computes network analytics measures. It will likely use a library such as NetworkX (a Python library for network analysis) to create a directed graph where each node is an address. The script reads through each transaction in the dataset and adds an edge from the sender node to the recipient node (possibly also storing the amount or token as edge attributes, if needed for further analysis). Once the graph is built, network_analysis.py will calculate metrics like the degree of each node (how many connections each address has). It may list the top addresses by out-degree (top senders) and top by in-degree (top receivers). The script can also identify connected components in the graph – subsets of the network where any address can reach any other through some chain of transactions. If the dataset is large, there might be one giant component and many tiny ones; the script can report the size of the largest component and how many isolated or small components exist. Additionally, the script can compute centrality measures: for example, betweenness centrality (which addresses often sit on the shortest paths between others) or PageRank/eigenvector centrality (which addresses are most “influential” when considering the network structure). It will highlight important nodes discovered by these metrics. The output could be text like: “Address JKL has the highest out-degree (connected to 120 other addresses as a sender)” or “The network has 3 major clusters of activity; the largest connected component contains 90% of the addresses, indicating most addresses are part of one big network.” If visualization is desired, this script could also generate a plot of the network or sub-network (though plotting might be done in a notebook environment if not directly in the script). In summary, network_analysis.py translates the linear transaction data into a graph format and extracts insights about the structure of the transaction network, helping identify key hubs and the overall connectivity of addresses on the blockchain ￼.

Each of these scripts works together as part of the pipeline: we load and clean the data, derive useful features, cluster the addresses to find behavioral groups, detect any anomalies, and finally analyze the network structure of transactions. By following this structured approach, even someone new to blockchain data science can systematically explore the dataset and uncover meaningful patterns and insights. The separation into scripts and steps also makes the process easier to understand, since each piece tackles a clear sub-problem (data prep, feature engineering, clustering, etc.) in the overall project. With this plan, one can proceed to write the actual code for each step, confident about what needs to be done at each stage and why it is important.

Sources: Blockchain data project description ￼ ￼ ￼ ￼ ￼; Concepts of clustering and anomaly detection ￼ ￼; Network analysis context from blockchain analytics ￼.